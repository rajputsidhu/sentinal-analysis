"""
Sentinel-AI â€” Module 10: Explainability Generator
Generates human-readable explanation for the dashboard.
"""

from openai import AsyncOpenAI
from app.config import settings
from app.models.schemas import RiskAnalysis
from app.utils.logger import log


EXPLAIN_PROMPT_TEMPLATE = """Explain in simple terms why this prompt was classified as {risk_level}.
Highlight dangerous segments and describe intent evolution if any.

User Prompt: {prompt}
Risk Level: {risk_level}
Risk Score: {score}/100
Attack Category: {category}
Drift Score: {drift}

Red-Team Assessment: {red_team_reasoning}
Blue-Team Explanation: {blue_team_explanation}

Provide a clear 2-3 sentence explanation suitable for a security dashboard."""


async def generate_explanation(prompt: str, risk_analysis: RiskAnalysis) -> str:
    """
    Generate a human-readable explanation of why a prompt was flagged.
    Uses LLM when available, falls back to template-based explanation.
    """
    if settings.use_llm:
        return await _llm_explain(prompt, risk_analysis)
    return _heuristic_explain(prompt, risk_analysis)


async def _llm_explain(prompt: str, risk_analysis: RiskAnalysis) -> str:
    """Use LLM to generate an explanation."""
    client = AsyncOpenAI(api_key=settings.openai_api_key)

    explain_prompt = EXPLAIN_PROMPT_TEMPLATE.format(
        risk_level=risk_analysis.blue_team.risk_level,
        prompt=prompt[:300],
        score=risk_analysis.final_score,
        category=risk_analysis.blue_team.attack_category,
        drift=f"{risk_analysis.drift.score:.2f} ({risk_analysis.drift.interpretation})",
        red_team_reasoning=risk_analysis.red_team.exploitation_strategy,
        blue_team_explanation=risk_analysis.blue_team.explanation,
    )

    try:
        response = await client.chat.completions.create(
            model=settings.openai_model,
            messages=[
                {"role": "user", "content": explain_prompt},
            ],
            temperature=0.3,
            max_tokens=200,
        )
        explanation = response.choices[0].message.content.strip()
        log.debug("Explanation generated by LLM")
        return explanation

    except Exception as e:
        log.error(f"Explanation LLM failed", error=str(e))
        return _heuristic_explain(prompt, risk_analysis)


def _heuristic_explain(prompt: str, risk_analysis: RiskAnalysis) -> str:
    """Template-based explanation fallback."""
    score = risk_analysis.final_score
    action = risk_analysis.action
    category = risk_analysis.blue_team.attack_category
    drift_info = risk_analysis.drift
    risky_phrases = risk_analysis.blue_team.risky_phrases

    if action == "allow":
        return f"This prompt appears safe with a risk score of {score:.0f}/100. No malicious intent patterns were detected."

    parts = []

    # Risk level
    parts.append(f"This prompt was classified as **{risk_analysis.blue_team.risk_level}** with a risk score of {score:.0f}/100.")

    # Attack type
    if category != "none":
        parts.append(f"Primary attack category: **{category.replace('_', ' ')}**.")

    # Risky phrases
    if risky_phrases:
        quoted = ", ".join(f'"{p}"' for p in risky_phrases[:3])
        parts.append(f"Suspicious segments detected: {quoted}.")

    # Drift
    if drift_info.score > 0.2:
        parts.append(f"Intent drift of {drift_info.score:.2f} detected ({drift_info.interpretation}), indicating conversation topic shifting.")

    # Action
    action_desc = {
        "warn": "A warning has been attached to this response.",
        "rewrite": "The prompt has been sanitized before forwarding to the main LLM.",
        "block": "The prompt has been blocked and will not be forwarded.",
    }
    parts.append(action_desc.get(action, ""))

    return " ".join(parts)
